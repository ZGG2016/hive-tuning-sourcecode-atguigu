
在 `03_读取HQL语句分类解析.md` 中，根据在控制台输入的内容，使用不同的解析方案，这里开始分析输入普通的sql语句。

这里先分析执行语句后，输出的打印内容，参考如下内容：

```shell
hive (default)> select * from test;
select * from test   【第一条】
OK                   【第二条】
test.id              【第三条】
1                    【第四条】
Time taken: 4.537 seconds, Fetched: 1 row(s)   【第五条】
```

```java
public class CliDriver {
    /*
            调用 Driver 的 run 方法执行sql语句，
            并设置执行sql语句后，控制台的输出内容
     */
    int processLocalCmd(String cmd, CommandProcessor proc, CliSessionState ss) {
        boolean escapeCRLF = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CLI_PRINT_ESCAPE_CRLF);
        int ret = 0;

        if (proc != null) {
            if (proc instanceof IDriver) {
                IDriver qp = (IDriver) proc;
                PrintStream out = ss.out;
                long start = System.currentTimeMillis();
                // verbose模式下，会打印执行的sql语句  【第一条】
                if (ss.getIsVerbose()) {
                    out.println(cmd);
                }
                // 执行sql语句
                ret = qp.run(cmd).getResponseCode();
                if (ret != 0) {
                    qp.close();
                    return ret;
                }

                // query has run capture the time
                long end = System.currentTimeMillis();
                double timeTaken = (end - start) / 1000.0;

                ArrayList<String> res = new ArrayList<String>();

                // 打印列名 【第三条】
                printHeader(qp, out);
                
                // print the results
                int counter = 0;
                try {
                    if (out instanceof FetchConverter) {
                        ((FetchConverter) out).fetchStarted();
                    }
                    while (qp.getResults(res)) {
                        for (String r : res) {
                            if (escapeCRLF) {
                                r = EscapeCRLFHelper.escapeCRLF(r);
                            }
                            // 打印sql语句执行结果  【第四条】
                            out.println(r);
                        }
                        // 输出结果的行数的 计数
                        counter += res.size();
                        res.clear();
                        if (out.checkError()) {
                            break;
                        }
                    }
                } catch (IOException e) {
                    console.printError("Failed with exception " + e.getClass().getName() + ":" + e.getMessage(),
                            "\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
                    ret = 1;
                }

                qp.close();

                if (out instanceof FetchConverter) {
                    ((FetchConverter) out).fetchFinished();
                }
                // 【第五条】
                console.printInfo(
                        "Time taken: " + timeTaken + " seconds" + (counter == 0 ? "" : ", Fetched: " + counter + " row(s)"));
            } else {
                String firstToken = tokenizeCmd(cmd.trim())[0];
                String cmd_1 = getFirstCmd(cmd.trim(), firstToken.length());

                if (ss.getIsVerbose()) {
                    ss.out.println(firstToken + " " + cmd_1);
                }
                CommandProcessorResponse res = proc.run(cmd_1);
                if (res.getResponseCode() != 0) {
                    ss.out
                            .println("Query returned non-zero code: " + res.getResponseCode() + ", cause: " + res.getErrorMessage());
                }
                if (res.getConsoleMessages() != null) {
                    for (String consoleMsg : res.getConsoleMessages()) {
                        console.printInfo(consoleMsg);
                    }
                }
                ret = res.getResponseCode();
            }
        }

        return ret;
    }
}
```

在 `07_解析AST.md` 中，生成了逻辑计划和物理计划，并分别进行了优化，生成了最终的执行计划。

下面就需要提交任务了。


在 `05_进入编译HQL代码.md` 中的 runInternal 方法中，执行完编译后，就开始执行sql

```java
public class Driver implements IDriver {
    private void runInternal(String command, boolean alreadyCompiled) throws CommandProcessorResponse {
        // ....
            try {
                if (!alreadyCompiled) {
                    // 开始编译
                    compileInternal(command, true);
                } 
                //....
                try {
                    // 执行sql
                    execute();
                } 
            }
        }
}
```

```java
public class Driver implements IDriver {

	private void execute() throws CommandProcessorResponse {

		// setQueryDisplays -> 对每个任务执行 task.setQueryDisplay(queryDisplay);
		setQueryDisplays(plan.getRootTasks());
	     int mrJobs = Utilities.getMRTasks(plan.getRootTasks()).size();
	     int jobs = mrJobs + Utilities.getTezTasks(plan.getRootTasks()).size()
	          + Utilities.getSparkTasks(plan.getRootTasks()).size();
	     if (jobs > 0) {
	        logMrWarning(mrJobs);
	        console.printInfo("Query ID = " + queryId);
	        console.printInfo("Total jobs = " + jobs);
	     }


	     DriverContext driverCxt = new DriverContext(ctx);

	    // Add root Tasks to runnable
      	for (Task<? extends Serializable> tsk : plan.getRootTasks()) {
	        // This should never happen, if it does, it's a bug with the potential to produce
	        // incorrect results.
	        assert tsk.getParentTasks() == null || tsk.getParentTasks().isEmpty();
	        // 添加到一个队列里 ConcurrentLinkedQueue
	        driverCxt.addToRunnable(tsk);

	        if (metrics != null) {
	          tsk.updateTaskMetrics(metrics);
	        }
      	}

      	// Loop while you either have tasks running, or tasks queued up
      	while (driverCxt.isRunning()) {
	        // Launch upto maxthreads tasks
	        Task<? extends Serializable> task;
	        // 遍历、取出、启动每个任务
	        while ((task = driverCxt.getRunnable(maxthreads)) != null) {
	          // 2.启动任务
	          TaskRunner runner = launchTask(task, queryId, noName, jobname, jobs, driverCxt);
	          if (!runner.isRunning()) {
	            break;
	          }
	        } 

	        //....
	    }

	    //....
	    // 打印结果中最后的 OK
	    if (console != null) {
	      console.printInfo("OK");
	    }
	}
}
```

```java
public class Driver implements IDriver {
  /**
   * Launches a new task
   *
   * @param tsk
   *          task being launched
   * @param queryId
   *          Id of the query containing the task
   * @param noName
   *          whether the task has a name set
   * @param jobname
   *          name of the task, if it is a map-reduce job
   * @param jobs
   *          number of map-reduce jobs
   * @param cxt
   *          the driver context
   */
  private TaskRunner launchTask(Task<? extends Serializable> tsk, String queryId, boolean noName,
      String jobname, int jobs, DriverContext cxt) throws HiveException {
    if (SessionState.get() != null) {
      SessionState.get().getHiveHistory().startTask(queryId, tsk, tsk.getClass().getName());
    }
    if (tsk.isMapRedTask() && !(tsk instanceof ConditionalTask)) {
      if (noName) {
        conf.set(MRJobConfig.JOB_NAME, jobname + " (" + tsk.getId() + ")");
      }
      conf.set(DagUtils.MAPREDUCE_WORKFLOW_NODE_NAME, tsk.getId());
      Utilities.setWorkflowAdjacencies(conf, plan);
      cxt.incCurJobNo(1);
      console.printInfo("Launching Job " + cxt.getCurJobNo() + " out of " + jobs);
    }
    tsk.initialize(queryState, plan, cxt, ctx.getOpContext());

    // 封装成一个线程
    TaskRunner tskRun = new TaskRunner(tsk);

    // 把它添加到一个 LinkedBlockingQueue 队列
    cxt.launching(tskRun);
    // Launch Task
    // 并行执行
    if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.EXECPARALLEL) && tsk.canExecuteInParallel()) {
      // Launch it in the parallel mode, as a separate thread only for MR tasks
      if (LOG.isInfoEnabled()){
        LOG.info("Starting task [" + tsk + "] in parallel");
      }
      // 1.1节 启动线程（任务）
      //可并行任务启动,实际上还是执行 tskRun.runSequential();
      tskRun.start();
    } 
    // 串行执行
    else {
      if (LOG.isInfoEnabled()){
        LOG.info("Starting task [" + tsk + "] in serial mode");
      }
      //不可并行任务,则按照序列顺序执行任务
      tskRun.runSequential();
    }
    return tskRun;
  }
}
```

## 1.1 tskRun.start()

启动线程，实际执行的是 run 方法

```java
public class TaskRunner extends Thread {

  @Override
  public void run() {
    runner = Thread.currentThread();
    try {
      SessionState.start(ss);
      // 这里
      runSequential();
    } finally {
      //....
    }
  }
}
```

```java
public class TaskRunner extends Thread {

 /**
   * Launches a task, and sets its exit value in the result variable.
   */
  public void runSequential() {
    int exitVal = -101;
    try {
    	// 这里
      exitVal = tsk.executeTask(ss == null ? null : ss.getHiveHistory());
    } catch (Throwable t) {
      //....
    }
    result.setExitVal(exitVal);
    if (tsk.getException() != null) {
      result.setTaskError(tsk.getException());
    }
  }	
}
```

```java
public abstract class Task<T extends Serializable> implements Serializable, Node {

 /**
   * This method is called in the Driver on every task. It updates counters and calls execute(),
   * which is overridden in each task
   * 
   * 在 Driver 中的每个任务上调用它
   * 
   *
   * @return return value of execute()
   */
  public int executeTask(HiveHistory hiveHistory) {
    try {
      this.setStarted();
      if (hiveHistory != null) {
        hiveHistory.logPlanProgress(queryPlan);
      }
      // 这里
      int retval = execute(driverContext);
      this.setDone();
      if (hiveHistory != null) {
        hiveHistory.logPlanProgress(queryPlan);
      }
      return retval;
    } catch (IOException e) {
      throw new RuntimeException("Unexpected error: " + e.getMessage(), e);
    }
  }
}	
```

```java
/**
 * ExecDriver is the central class in co-ordinating execution of any map-reduce task.
 * It's main responsibilities are:
 *
 * - Converting the plan (MapredWork) into a MR Job (JobConf)
 * - Submitting a MR job to the cluster via JobClient and ExecHelper
 * - Executing MR job in local execution mode (where applicable)
 * 
 * 主要负责：
 * - 将计划转成MR Job
 * - 通过 JobClient 和 ExecHelper, 将 MR job 提交到集群
 * - 以本地模式，执行 MR job
 *
 */
public class ExecDriver extends Task<MapredWork> implements Serializable, HadoopJobExecHook {

 /**
   * Execute a query plan using Hadoop.
   */
  @SuppressWarnings({"deprecation", "unchecked"})
  @Override
  public int execute(DriverContext driverContext) {
  	//...
  	// mr临时工作目录
  	emptyScratchDir = ctx.getMRTmpPath();
    FileSystem fs = emptyScratchDir.getFileSystem(job);
    fs.mkdirs(emptyScratchDir);

    job.setMapperClass(ExecMapper.class);
    job.setPartitionerClass(JavaUtils.loadClass(partitioner));
    job.setReducerClass(ExecReducer.class);
    //...

    jc = new JobClient(job);

    // 提交job
    rj = jc.submitJob(job);

    //...
  }
}
```